{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1CkEaDDXAk9toO1JVYE3PL4qTaaUcPMDi","authorship_tag":"ABX9TyP6zB7/zAnqU/ER7BMIJTtJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import pprint\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import plot_tree\n","import matplotlib.pyplot as plt\n","\n","class ID3DecisionTree:\n","    \"\"\"\n","    Decision tree using ID3\n","    \"\"\"\n","    def __init__(self):\n","        pass\n","\n","    def get_entropy(self, df):\n","        # TODO: Calculate the entropy(s)\n","        class_values, class_counts = np.unique(df['play'], return_counts=True)\n","        probabilities = class_counts / len(df)\n","        entropy = -np.sum(probabilities * np.log2(probabilities))\n","        return entropy\n","\n","    def split_table(self, df, attr, value):\n","        return df[df[attr] == value].reset_index(drop=True)\n","\n","    def fit(self, df):\n","\n","        tree={}\n","        # compute entropy(s)\n","        entropy_s = self.get_entropy(df)\n","\n","        # List of all attrs except the class\n","        attrs = df.keys()[:-1]\n","\n","        max_gain = 0\n","        max_gain_attr = None\n","        # for all attr, find the attr with max gain\n","        for attr in attrs: #Outlook,Temparature,Humidity,Wind\n","            values = df[attr].unique()\n","            # TODO: Calculate the entropies of unique values of attr\n","            # Example: attr=Outlook has entropies = [0.971, 0.971, 0]\n","            entropies = []\n","\n","            # TODO: Calculate the probabilities of unique values of attr\n","            # Example: attr=Outlook has probabilities = [5/14, 5/14, 0]\n","            probabilities = []\n","\n","            for value in values:  #Sunny,Rain,Overcast (Outlook), same for others\n","              subset=df[df[attr]==value]\n","              subset_entropy=self.get_entropy(subset)\n","              entropies.append(subset_entropy)\n","              probabilities.append(len(subset)/len(df))\n","\n","\n","            # Calculate the average information entropy\n","            average_info_entropy = 0\n","            for probability, entropy in zip(probabilities, entropies):\n","                average_info_entropy += probability * entropy\n","\n","            # TODO: Calculate attr gain\n","            attr_gain = entropy_s - average_info_entropy\n","\n","\n","            # TODO: Update the max_gain_attr\n","            if attr_gain > max_gain:\n","                max_gain = attr_gain\n","                max_gain_attr = attr\n","\n","        tree[max_gain_attr] = {}\n","        # Split the df based on the values of max_gain_attr\n","        values = df[max_gain_attr].unique()\n","        for value in values:\n","            new_df = self.split_table(df, max_gain_attr, value)\n","            class_values, class_counts = np.unique(new_df['play'],return_counts=True)\n","\n","            # If it is a pure class, then this is the leaf node\n","            # else divide the new_df further\n","            if len(class_counts)==1:\n","                tree[max_gain_attr][value] = class_values[0]\n","            else:\n","                tree[max_gain_attr][value] = self.fit(new_df)\n","\n","        return tree\n","\n","    def predict(self, example, tree, default=None):\n","        attribute = next(iter(tree))\n","        if example[attribute] in tree[attribute].keys():\n","            subtree = tree[attribute][example[attribute]]\n","            if isinstance(subtree, dict):\n","                return self.predict(example, subtree)\n","            else:\n","                return subtree\n","        else:\n","            return default\n","\n","    def evaluate(self, tree, df):\n","        # TODO: Complete the evaluate method\n","        correct_predictions = 0\n","        for index, row in df.iterrows():\n","            example = row.drop('play').to_dict()\n","            prediction = self.predict(example, tree)\n","            if prediction == row['play']:\n","                correct_predictions += 1\n","        accuracy = correct_predictions / len(df) * 100\n","        return accuracy\n","\n","# TODO: Complete the class for decision tree using CART\n","class CARTDecisionTree:\n","    def __init__(self):\n","        self.tree = None\n","\n","    def fit(self, df):\n","        self.tree = self.build_tree(df, 'play')\n","        return self.tree\n","\n","    def gini_impurity(self, labels):\n","        # Calculate Gini impurity for a set of labels\n","        if len(labels) == 0:\n","            return 0.0\n","\n","        # Count occurrences of each class\n","        class_counts = labels.value_counts()\n","\n","        # Calculate Gini impurity\n","        total_samples = len(labels)\n","        impurity = 1.0\n","        for count in class_counts:\n","            proportion = count / total_samples\n","            impurity -= proportion ** 2\n","\n","        return impurity\n","\n","    def find_best_split(self, df, target_column):\n","        # Find the best attribute to split on based on Gini impurity\n","        best_split_attr = None\n","        best_split_value = None\n","        min_gini = float('inf')\n","\n","        for attribute in df.columns:\n","            if attribute == target_column:\n","                continue\n","\n","            # Get unique values of the attribute\n","            values = df[attribute].unique()\n","\n","            for value in values:\n","                # Split the dataset\n","                left_subset = df[df[attribute] <= value]\n","                right_subset = df[df[attribute] > value]\n","\n","                # Calculate Gini impurity for the split\n","                left_impurity = self.gini_impurity(left_subset[target_column])\n","                right_impurity = self.gini_impurity(right_subset[target_column])\n","\n","                # Weighted sum of impurities\n","                weighted_impurity = (len(left_subset) / len(df)) * left_impurity \\\n","                                    + (len(right_subset) / len(df)) * right_impurity\n","\n","                # Update best split if the current split is better\n","                if weighted_impurity < min_gini:\n","                    min_gini = weighted_impurity\n","                    best_split_attr = attribute\n","                    best_split_value = value\n","\n","        return best_split_attr, best_split_value\n","    def build_tree(self, df, target_column):\n","        # Base case: If all examples have the same label, create a leaf node\n","          if len(df[target_column].unique()) == 1:\n","            return {'class': df[target_column].iloc[0]}\n","\n","        # Find the best attribute and value to split on\n","          best_split_attr, best_split_value = self.find_best_split(df, target_column)\n","\n","        # If no split improves purity, create a leaf node\n","          if best_split_attr is None:\n","            return {'class': df[target_column].mode().iloc[0]}\n","\n","        # Split the dataset\n","          left_subset = df[df[best_split_attr] <= best_split_value]\n","          right_subset = df[df[best_split_attr] > best_split_value]\n","\n","        # Recursively build left and right subtrees\n","          left_subtree = self.build_tree(left_subset, target_column)\n","          right_subtree = self.build_tree(right_subset, target_column)\n","\n","        # Return the current node with split information and subtrees\n","          return {'attribute': best_split_attr,\n","                  'value': best_split_value,\n","                  'left': left_subtree,\n","                  'right': right_subtree}\n","\n","\n","    def predict(self, example, tree):\n","        # Predict the class for a single example\n","        # Recursive function to predict the class for a single example\n","        if 'class' in tree:\n","            return tree['class']\n","        else:\n","            attr_value = example[tree['attribute']]\n","            if attr_value <= tree['value']:\n","                return self.predict(example, tree['left'])\n","            else:\n","                return self.predict(example, tree['right'])\n","\n","    def evaluate(self, tree, df, target_column='play'):\n","        # Evaluate the accuracy of the trained tree on the given DataFrame\n","        examples = df.drop(target_column, axis=1)\n","        true_labels = df[target_column]\n","\n","        predictions = [self.predict(example, tree) for _, example in examples.iterrows()]\n","\n","        # Calculate accuracy\n","        correct_predictions = sum(pred == true_label for pred, true_label in zip(predictions, true_labels))\n","        accuracy = correct_predictions / len(true_labels) * 100\n","        return accuracy\n","\n","# Read the dataset\n","# data_path = '/content/drive/MyDrive/Data Mining Lab/finaldataset.csv'\n","data_path = '/content/drive/MyDrive/MlLab/play_tennis.csv'\n","\n","\n","df = pd.read_csv(data_path)\n","# print(df)\n","\n","# TODO: Shuffle the df randomly\n","df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n","# print(df)\n","\n","# TODO: Split the df into train_df and test_df using 80:20 ratio\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","train_df = train_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","# print(train_df)\n","# print(test_df)\n","\n","# Train the model\n","model = ID3DecisionTree()\n","\n","tree = model.fit(train_df)\n","\n","# Visualize the decision tree\n","print('the decision tree for ID3 algorithm:\\n')\n","pprint.pprint(tree)\n","\n","\n","# Predict an example\n","x = {'Outlook': 'Rain', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak'}\n","y_pred = model.predict(x, tree)\n","print(\"#ID3 algorithm: Output class:\", y_pred)\n","\n","# Evaluate the model\n","acc = model.evaluate(tree, test_df)\n","print(\"#ID3 algorithm: Accuracy: {:.3f}\".format(acc))\n","\n","\n","\n","#Train model\n","model = CARTDecisionTree()\n","\n","# Fit the tree on the training data\n","# Assuming 'target' is the name of the target variable column\n","tree = model.fit(train_df)\n","print('\\n The decision tree for CART:\\n')\n","# Visualize the decision tree\n","pprint.pprint(tree)\n","\n","# Predict an example\n","x = {'Outlook': 'Rain', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak'}\n","y_pred = model.predict(x, tree)\n","print(\"#CART algorithm: Output class:\", y_pred)\n","\n","# Evaluate the model\n","acc = model.evaluate(tree, test_df, target_column='play')\n","print(\"#CART algorithm: Accuracy: {:.3f}\".format(acc))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JdijmCPnSK1r","executionInfo":{"status":"ok","timestamp":1704795640447,"user_tz":-360,"elapsed":446,"user":{"displayName":"Tama Shil","userId":"07589384387683053997"}},"outputId":"4cea9be4-3d9d-4423-c111-7348f4c9191e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["the decision tree for ID3 algorithm:\n","\n","{'Outlook': {'Overcast': 'Yes',\n","             'Rain': {'Wind': {'Strong': 'No', 'Weak': 'Yes'}},\n","             'Sunny': {'Temperature': {'Cool': 'Yes',\n","                                       'Hot': 'No',\n","                                       'Mild': 'No'}}}}\n","#ID3 algorithm: Output class: Yes\n","#ID3 algorithm: Accuracy: 66.667\n","\n"," The decision tree for CART:\n","\n","{'attribute': 'Outlook',\n"," 'left': {'class': 'Yes'},\n"," 'right': {'attribute': 'Wind',\n","           'left': {'class': 'No'},\n","           'right': {'attribute': 'Outlook',\n","                     'left': {'class': 'Yes'},\n","                     'right': {'attribute': 'Temperature',\n","                               'left': {'class': 'Yes'},\n","                               'right': {'class': 'No'},\n","                               'value': 'Cool'},\n","                     'value': 'Rain'},\n","           'value': 'Strong'},\n"," 'value': 'Overcast'}\n","#CART algorithm: Output class: Yes\n","#CART algorithm: Accuracy: 66.667\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VEneMFbMuSc0","executionInfo":{"status":"ok","timestamp":1704795464091,"user_tz":-360,"elapsed":4145,"user":{"displayName":"Tama Shil","userId":"07589384387683053997"}},"outputId":"fbea8414-0fff-48ab-9838-6e979852f624"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]}]}